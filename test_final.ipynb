{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c2bb15c-c1c8-486e-86bb-b012ce64d7c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !wget --header=\"Host: doc-0s-54-docs.googleusercontent.com\" --header=\"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Cookie: AUTH_prqi0ddebcnouinrgq4n8k579qees3ri=09250325709264443050|1674244950000|1c1ogi62gqa0o073nvbr2kl2f6fikp72\" --header=\"Connection: keep-alive\" \"https://doc-0s-54-docs.googleusercontent.com/docs/securesc/5n63bj25lj97nngplfo91cqd1qhl5kn0/8ej607icfh0m6j3fb5tj6udnh0la5itf/1674245025000/06977169721700109230/09250325709264443050/1z2lFY4SWkYU82vMtX1nYaMQZwU0a4HvR?e=download&ax=ALjR8sxbmW2AJUtlvX8dw48uRbOUdNPAYG91QPtqFSN38R7BL3ClSGdI95QBmCoOWDZBbub-fCLCrKdUtwrWWqTJjWzsCw28oxWDeF1ad7L3L-Dn_NpnD7EVG8hRCVjjyHhg5kczrEjIhUGJFgkCTXSdFPLALYTFvRHD7GEWxJul0QXLUm2hrMJjIhCQKkmLX39B6iO-HTEXRbw0OgPLZqkpMVwZyrtpWecRAir_yYqkulr2sPZ21TS30XoazMqRU7yfsKfqLLeMmGyc4O6Qa1bxBhp4nhwE6EjlenB1MDnN3zEPMctoUq6tCBvezYC6nQme-VIbhTdLDhKvBHp_VcDZ23KHDp_UlvPMPQkIBmXS_ihZRHyv2c1N_Zq5SJ2Z8iRJuNQoFYMTQbZNQA657HEa30uIupKQVDBPmb1k1Hgm5lkPfUP9zaWKWd77i4fd7ZyG1fnRamXiHYyHV2Kb9XXJt1KvBgKbv72aX_qDWwbRF1Xiy_lmGMYQ68GE0pRSRmhDnu0j8wllqYS7gbHuWuNqRFyHz_d4otycw0kMvVSV_dQkHcU04y1_tCIwBfL2xZixJJ-0qIE8ns-sTCLDPb0C_tNxufUDrQoYohqdyOHLfpLPrH9TDgtcUahBRj7TpFrP4MwuaILpYF-Pjbj5ifg1e8xyjdFODzlHTg0RWeqeaGWjGIh22OidVACNEFPwCClrZOBU7qx3W5Sn8VSBBWFLtSC3mGSoKCaryqmiGmeoqTwaAwz5_2rhaB-s-NjdLDXpc88cyqNhVsgueMVfFfdf_BK1shiqV_UGlsg1teXgPXeMSm03HxU1kWaT01_Qnm7lRT8WdZNXd8ndFObio_BAY5y8CEydet7gsoRd4cHTIuqs7EIB02faHSJIKkvVBqXY3ZtPGgkc3hjKGShU1ta-UfS2hEE4Sx_9NCjBYvoEJHuESI_l8PaSFY839dS8jKc0oM0umPPLS4SgkX4ORyL4DtkYmHVv0luOMSc&uuid=66b0f1f1-2bd2-4c63-8fba-60123d3f2d1c&authuser=2\" -c -O 'csebuetnlp-banglabert_large.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3b34628-7137-4154-bcb8-5cdc7718f100",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: 'keyring.backends'\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.25.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (5.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "!pip install transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "873292c8-0256-4348-862e-5978509f5cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format(file):\n",
    "  with open(file) as f:\n",
    "      # read csv file as pandas dataframe\n",
    "      df = pd.read_csv(f, sep=',', \n",
    "                       # names=['Sentence_id', 'Word', 'Tag']\n",
    "                      )\n",
    "      # group by sentence_id and create list of words and list of tags\n",
    "      df = df.groupby('sentence_id').agg({'word': lambda x: list(x), 'tag': lambda x: list(x)})\n",
    "      # create pair of 2nd and 3rd column (word , tag)\n",
    "      df['word_tag'] = df.apply(lambda x: list(zip(x['word'], x['tag'])), axis=1)\n",
    "      # create list of pair of 2nd and 3rd column (word , tag)\n",
    "      word_tag_list = df['word_tag'].tolist()\n",
    "      \n",
    "      return word_tag_list\n",
    "\n",
    "file = 'train_generic.csv'\n",
    "samples = format(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "144e8e6c-8dda-4654-a5cf-7d59da3b3fb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('তার', 'O'), ('মৃত্যুর', 'O'), ('দশ', 'O'), ('দিন', 'O'), ('পর,', 'O'), ('১১৫', 'O'), ('কৃষ্ণাঙ্গ', 'O'), ('উচ্চ', 'O'), ('বিদ্যালয়ের', 'O'), ('শিক্ষার্থীরা', 'O'), ('তার', 'O'), ('হত্যার', 'O'), ('প্রতিবাদে', 'O'), ('ম্যাককম্ব', 'B-LOC'), ('এর', 'O'), ('মাধ্যমে', 'O'), ('মিছিল', 'O'), ('করেছে।', 'O')], [('ব্রাংম্যান', 'O'), ('ডাম্পসন', 'O'), ('১৪০০', 'O'), ('সালে', 'O'), ('আন্তর্জাতিক', 'B-GRP'), ('রেড', 'I-GRP'), ('ক্রস', 'I-GRP'), ('ও', 'I-GRP'), ('রেড', 'I-GRP'), ('ক্রিসেন্ট', 'I-GRP'), ('আন্দোলন', 'I-GRP'), ('এর', 'O'), ('স্বেচ্ছাসেবক', 'O'), ('হিসেবে', 'O'), ('শুরু', 'O'), ('করেছিলেন।', 'O')], [('রাজকীয়', 'O'), ('বাসস্থান', 'O'), ('থেকে', 'O'), ('রাজ্যের', 'O'), ('মন্দির', 'O'), ('পর্যন্ত', 'O'), ('স্বল্প', 'O'), ('দূরত্ব', 'O'), ('ভ্রমণ', 'O'), ('করে,', 'O'), ('রাজা', 'O'), ('মিছিলে', 'O'), ('অনেক', 'O'), ('ধারক', 'O'), ('নিয়ে', 'O'), ('পালকি', 'B-PROD'), ('বসবেন।', 'O')], [('তিনি', 'O'), ('তৃতীয়', 'O'), ('সহস্রাব্দে', 'O'), ('গ্র্যান্ড', 'B-CW'), ('ওলে', 'I-CW'), ('অপ্রি', 'I-CW'), ('এ', 'O'), ('অন্তর্ভুক্ত', 'O'), ('হওয়া', 'O'), ('প্রথম', 'O'), ('ব্যক্তি', 'O'), ('হয়েছিলেন।', 'O')], [('প্রায়', 'O'), ('১৮,০০০', 'O'), ('এর', 'O'), ('উচ্চতর', 'O'), ('অনুপাত', 'O'), ('দোআঁশ', 'B-PROD'), ('মাটিতে', 'O'), ('পরিমাপ', 'O'), ('করা', 'O'), ('হয়েছিল।', 'O')]]\n"
     ]
    }
   ],
   "source": [
    "print(samples[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f38b873-b3a3-4b1a-89ea-e13ec37ee424",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = ['_'] + sorted({tag for sentence in samples \n",
    "                             for _, tag in sentence})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8346e820-e71c-4842-b06d-104f37d33e45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', 'B-CORP', 'B-CW', 'B-GRP', 'B-LOC', 'B-PER', 'B-PROD', 'I-CORP', 'I-CW', 'I-GRP', 'I-LOC', 'I-PER', 'I-PROD', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79dbeca0-59d6-4765-90b0-a6238104c0bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFElectraForTokenClassification.\n",
      "\n",
      "All the layers of TFElectraForTokenClassification were initialized from the model checkpoint at data/csebuetnlp-banglabert_large_f1_81.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFElectraForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_electra_for_token_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " electra (TFElectraMainLayer  multiple                 335605760 \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_73 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  14350     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 335,620,110\n",
      "Trainable params: 335,620,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, TFAutoModelForTokenClassification , AutoTokenizer\n",
    "\n",
    "#MODEL_NAME = 'bert-base-cased' \n",
    "# MODEL_NAME = \"csebuetnlp/banglabert\"\n",
    "MODEL_NAME = \"csebuetnlp/banglabert_large\"\n",
    "\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=len(schema)) \n",
    "\n",
    "#old_model = 'csebuetnlp-banglabert0.6987703481430697.h5'\n",
    "old_model = 'data/csebuetnlp-banglabert_large_f1_81.h5'\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(old_model, from_pt=False,\n",
    "                                                       config=config)\n",
    "                                          \n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.000001)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dff9475f-a457-4c3b-9974-5357973e118f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 743.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def format_test(file):\n",
    "  with open(file) as f:\n",
    "      # read csv file as pandas dataframe\n",
    "      df = pd.read_csv(f, sep=',', \n",
    "                       # names=['Sentence_id', 'Word', 'Tag']\n",
    "                      )\n",
    "      # group by sentence_id and create list of words and list of tags\n",
    "      df = df.groupby('sentence_id').agg({'word': lambda x: list(x)})\n",
    "      # create pair of 2nd and 3rd column (word , tag)\n",
    "      # df['word_tag'] = df.apply(lambda x: list(zip(x['word'], x['tag'])), axis=1)\n",
    "      # create list of pair of 2nd and 3rd column (word , tag)\n",
    "      word_list = df['word'].tolist()\n",
    "      \n",
    "      return word_list\n",
    "subtoken_map = {}\n",
    "def tokenize_sample(sample):\n",
    "    # seq = [\n",
    "    #            (subtoken, tag)\n",
    "    #            for token, tag in sample\n",
    "    #            for subtoken in tokenizer(token)['input_ids'][1:-1]\n",
    "    #        ]\n",
    "    seq = []\n",
    "    sub_lengths = []\n",
    "               # (subtoken, tag)\n",
    "    for token in sample:\n",
    "        subtokens = tokenizer(token)['input_ids'][1:-1]\n",
    "        # subtoken_map\n",
    "        for subtoken in subtokens:\n",
    "            seq.append((subtoken))\n",
    "        sub_lengths.append(len(subtokens))\n",
    "           \n",
    "    return [(3)] + seq + [(4)], sub_lengths \n",
    "\n",
    "def preprocess(samples):\n",
    "    # tag_index = {tag: i for i, tag in enumerate(schema)}\n",
    "    # tokenized_samples = list(tqdm(map(tokenize_sample, samples)))\n",
    "    tokenized_samples = []\n",
    "    all_sub_lengths = []\n",
    "    for s in tqdm(samples):\n",
    "        seq, lengths = tokenize_sample(s)\n",
    "        tokenized_samples.append(seq)\n",
    "        all_sub_lengths.append(lengths)\n",
    "        \n",
    "        # print(lengths, sum(lengths))\n",
    "        # break\n",
    "        \n",
    "    max_len = max(map(len, tokenized_samples))\n",
    "    print(max_len)\n",
    "    X = np.zeros((len(samples), max_len), dtype=np.int32)\n",
    "    for i, sentence in enumerate(tokenized_samples):\n",
    "        for j, (subtoken_id) in enumerate(sentence):\n",
    "            X[i, j] = subtoken_id\n",
    "        \n",
    "    return X, all_sub_lengths\n",
    "\n",
    "# file = 'sample_test_set_no_punc.csv'\n",
    "file = ''\n",
    "dev_samples = format_test(file)\n",
    "X_valid, all_sub_lengths = preprocess(dev_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11ec2c0f-cb49-4d47-aef4-ade4aa1b0122",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 9s 9s/step\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# [loss, accuracy] = model.evaluate(X_valid, y_valid)\n",
    "# print(\"Loss:%1.3f, Accuracy:%1.3f\" % (loss, accuracy))\n",
    "\n",
    "# Find the f1 score of the model output\n",
    "from sklearn.metrics import f1_score\n",
    "y_pred = model.predict(X_valid).logits\n",
    "y_pred_raw = np.argmax(y_pred, axis=-1)\n",
    "print(type(y_pred_raw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af236b0a-5c67-4e33-b642-479a9aad2843",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 27)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0e647eb-4503-4ba3-8217-2044182053f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '_', 1: 'B-CORP', 2: 'B-CW', 3: 'B-GRP', 4: 'B-LOC', 5: 'B-PER', 6: 'B-PROD', 7: 'I-CORP', 8: 'I-CW', 9: 'I-GRP', 10: 'I-LOC', 11: 'I-PER', 12: 'I-PROD', 13: 'O'}\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "reverse_tag_index = {i:tag for i, tag in enumerate(schema)}\n",
    "print(reverse_tag_index)\n",
    "lens = []\n",
    "for s in dev_samples:\n",
    "    lens.append(len(s))\n",
    "    \n",
    "# print(lens)\n",
    "print(max(lens))\n",
    "cum_len = list(np.cumsum(lens))\n",
    "# print(cum_len)\n",
    "\n",
    "dev_pred_out = open('dev_pred_ours.txt', 'w')\n",
    "# y_pred = list(filter(lambda a: a!= 0, y_pred) )\n",
    "# print(y_pred[0])\n",
    "\n",
    "labels = []\n",
    "\n",
    "for i in range(y_pred_raw.shape[0]):\n",
    "    # print(y , reverse_tag_index[y])\n",
    "    index = 1\n",
    "    for sublen in all_sub_lengths[i]:\n",
    "        # print(reverse_tag_index[y_pred_raw[i][index]], file=dev_pred_out)\n",
    "        label = reverse_tag_index[y_pred_raw[i][index]]\n",
    "        labels.append(label)\n",
    "        \n",
    "        index += sublen\n",
    "        \n",
    "    # for j in range(1, lens[i] + 1):\n",
    "    #     # print(y_pred_raw[i, j])\n",
    "    #     print(reverse_tag_index[y_pred_raw[i][j]], file=dev_pred_out)\n",
    "        \n",
    "    # if (i + 1) in cum_len:\n",
    "    labels.append('')\n",
    "    \n",
    "    # print(file=dev_pred_out)\n",
    "    # break\n",
    "\n",
    "dev_pred_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1de48b9-163a-4aaf-a41f-f54106a873ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i, label in enumerate(labels):\n",
    "#     if i + 1 < len(labels):\n",
    "#         if len(labels[i]) > 0 and len(labels[i+1]) > 0:\n",
    "#             if labels[i][0] == \"B\" and labels[i+1][0] == \"B\":\n",
    "#                 labels[i+1][0] == \"I\"\n",
    "\n",
    "dev_pred_out = open('dev_pred_ours.txt', 'w')\n",
    "for i, label in enumerate(labels):\n",
    "    print(label, file=dev_pred_out)\n",
    "\n",
    "dev_pred_out.close()                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f0aaf8-14d6-485f-83ba-7dc956f6a57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "42a1355c-3054-4846-960a-de4ee8806add",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"P@GRP\": 0.7641509433962257,\n",
      "  \"R@GRP\": 0.6864406779661011,\n",
      "  \"F1@GRP\": 0.7232142857142353,\n",
      "  \"P@LOC\": 0.8315789473684202,\n",
      "  \"R@LOC\": 0.7821782178217814,\n",
      "  \"F1@LOC\": 0.8061224489795411,\n",
      "  \"P@CW\": 0.6929824561403503,\n",
      "  \"R@CW\": 0.6583333333333328,\n",
      "  \"F1@CW\": 0.6752136752136247,\n",
      "  \"P@PROD\": 0.7134502923976603,\n",
      "  \"R@PROD\": 0.6421052631578944,\n",
      "  \"F1@PROD\": 0.67590027700826,\n",
      "  \"P@PER\": 0.8815789473684204,\n",
      "  \"R@PER\": 0.9305555555555548,\n",
      "  \"F1@PER\": 0.9054054054053546,\n",
      "  \"P@CORP\": 0.7122302158273376,\n",
      "  \"R@CORP\": 0.7795275590551175,\n",
      "  \"F1@CORP\": 0.7443609022555886,\n",
      "  \"Precision\": 0.7644787644787644,\n",
      "  \"Recall\": 0.7424999999999999,\n",
      "  \"F1\": 0.7533291058972232,\n",
      "  \"MD@R\": 0.8425,\n",
      "  \"MD@P\": 0.8674388674388674,\n",
      "  \"MD@F1\": 0.8547875713379335,\n",
      "  \"ALLTRUE\": 800,\n",
      "  \"ALLRECALLED\": 674,\n",
      "  \"ALLPRED\": 777\n",
      "}\n",
      "Precision 0.76, Recall 0.74, F1 0.75\n"
     ]
    }
   ],
   "source": [
    "# !python eval_script/eval_main.py --true eval_script/dev_gt_labels.txt --pred dev_pred_ours.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7326c3-446d-486b-9046-b2678efee0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
